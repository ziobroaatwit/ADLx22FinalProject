{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**PUBG Finish Placement Model Creation**","metadata":{"id":"DLttDlMDwPoV"}},{"cell_type":"markdown","source":"What did we discover? \nNote: Some of the markdown cells in this notebook are generally notes for myself and thought process about tackling the data.","metadata":{}},{"cell_type":"markdown","source":"**Data Input**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrainpath='../input/pubg-finish-placement-prediction/train_V2.csv'\ntestpath='../input/pubg-finish-placement-prediction/test_V2.csv'\nsamplepath='../input/pubg-finish-placement-prediction/sample_submission_V2.csv'\n\ntrain = pd.read_csv(trainpath)\ntest = pd.read_csv(testpath)\nsample= pd.read_csv(samplepath)","metadata":{"id":"xlzvdk7FvRYG","execution":{"iopub.status.busy":"2022-08-13T02:01:47.485703Z","iopub.execute_input":"2022-08-13T02:01:47.486196Z","iopub.status.idle":"2022-08-13T02:02:28.749003Z","shell.execute_reply.started":"2022-08-13T02:01:47.486109Z","shell.execute_reply":"2022-08-13T02:02:28.747636Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"id":"ruiDTggSxUA9","outputId":"a34dca86-6631-400f-c150-85a28a0fd611","execution":{"iopub.status.busy":"2022-08-13T02:02:28.751101Z","iopub.execute_input":"2022-08-13T02:02:28.751523Z","iopub.status.idle":"2022-08-13T02:02:28.792859Z","shell.execute_reply.started":"2022-08-13T02:02:28.751482Z","shell.execute_reply":"2022-08-13T02:02:28.791789Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"id":"NGEi6QOQ07iL","outputId":"fc016df4-e009-4f62-f25b-dc9ee968cd22","execution":{"iopub.status.busy":"2022-08-13T02:02:28.794021Z","iopub.execute_input":"2022-08-13T02:02:28.794334Z","iopub.status.idle":"2022-08-13T02:02:28.822395Z","shell.execute_reply.started":"2022-08-13T02:02:28.794306Z","shell.execute_reply":"2022-08-13T02:02:28.821337Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Okay, so looking at the head of the data. I can already spot a few rows to get rid of since they are not useful for our purposes here. I think ID, GroupID, and Match ID are irrelevant to our predictions as they are essentially just labels to identify groups and players as well as the matches the stats came from. Additionally, there is the Match_Type, ideally this categorical variable should be turned into dummy variables -> Additional columns to mark 1 and 0. This is likely where my model is deficient!\n\nAdditionally there's quite a few anomalous values for rank points, with -1 values amongst 1500's or greater. This just seems not great, I could choose to drop it for time or attempt to regularize this. (I chose to drop it)","metadata":{"id":"DuxSoo2D1HkT"}},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"Removal of the various ID's, as well as filling a NaN in the index 2744604 as noted by a Kaggler named SeaOtter","metadata":{}},{"cell_type":"code","source":"train=train.fillna(method='ffill')\ntrain.drop(['Id','groupId','matchId'],inplace=True,axis=1)\ntest.drop(['Id','groupId','matchId'],inplace=True,axis=1)","metadata":{"id":"XoVhqqNv0-l8","execution":{"iopub.status.busy":"2022-08-13T02:02:28.825435Z","iopub.execute_input":"2022-08-13T02:02:28.825827Z","iopub.status.idle":"2022-08-13T02:02:31.841257Z","shell.execute_reply.started":"2022-08-13T02:02:28.825719Z","shell.execute_reply":"2022-08-13T02:02:31.839960Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"This was an attempt to make a heatmap of the correlations between the features and the goal. While mostly unreadable it does give a valuable look that there are actually a pretty significant number of variables just have close to no correlation to the final placement, but a select few that do have positive correlations. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sn\ncorrMatrix=train.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.savefig('heatmap.png')\nplt.show()\ncorrMatrix","metadata":{"execution":{"iopub.status.busy":"2022-08-13T02:02:31.842517Z","iopub.execute_input":"2022-08-13T02:02:31.842895Z","iopub.status.idle":"2022-08-13T02:02:44.808104Z","shell.execute_reply.started":"2022-08-13T02:02:31.842862Z","shell.execute_reply":"2022-08-13T02:02:44.806929Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We can see from the correlation matrix, the variables that have the largest correlation to winPlacePerc are boosts, walking distance, weapons acquired. ETC, So I'll toss the other uncorrelated values from the data and train on just those. Since I find that the others will be relatively useless bloat or bring down our model. The data set is properly enormous so if we can bring down the size of it that would be perfect.","metadata":{}},{"cell_type":"markdown","source":"Creating new sub-data sets out of the columns I think are best correlated to the goal, and creating the label set. Then splitting these smaller sets into train_test_split groups so I can cross validate within the notebook before submitting it. Additionally, ignoring the actual test data to instead use the CV data is just....computationally less annoying. Using the test data takes forever.","metadata":{}},{"cell_type":"code","source":"xtrain=train[['boosts','walkDistance','weaponsAcquired','damageDealt','kills']]\nytrain=train['winPlacePerc']\nxtest=test[['boosts','walkDistance','weaponsAcquired','damageDealt','kills']]\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(xtrain, ytrain, test_size=0.33, random_state=42)","metadata":{"id":"U4RQrukwDYXW","execution":{"iopub.status.busy":"2022-08-13T02:02:44.809645Z","iopub.execute_input":"2022-08-13T02:02:44.810700Z","iopub.status.idle":"2022-08-13T02:02:46.130252Z","shell.execute_reply.started":"2022-08-13T02:02:44.810641Z","shell.execute_reply":"2022-08-13T02:02:46.128817Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Here lies my final dense NN I left behind rather quickly. I tried many variations of the layers including different numbers of neurons, different amounts of layers, and variations of dropout. All very fruitless in producing good results for this dataset compared to Linear and Tree based models. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Flatten\ndef myNN(train_data):\n  model=tf.keras.models.Sequential()\n  model.add(layers.Dense(units=64,input_shape=(train_data.shape[1],),activation='relu',name='input'))\n  model.add(layers.Dropout(0.15))\n  model.add(layers.Dense(units=64,activation='relu',name='hidden1'))\n  model.add(layers.Dropout(0.15))\n  model.add(layers.Dense(units=64,activation='relu',name='hidden2'))\n  model.add(layers.Dropout(0.15))\n  model.add(layers.Dense(units=64,activation='relu',name='hidden3'))\n\n  model.add(layers.Dense(units=1,activation=None,name='output'))\n\n  model.compile(optimizer='rmsprop',loss='mse',metrics=['mae','accuracy'])\n\n  return model","metadata":{"id":"rg2N-X9q5kEt","execution":{"iopub.status.busy":"2022-08-13T02:02:46.132026Z","iopub.execute_input":"2022-08-13T02:02:46.132847Z","iopub.status.idle":"2022-08-13T02:02:46.146458Z","shell.execute_reply.started":"2022-08-13T02:02:46.132788Z","shell.execute_reply":"2022-08-13T02:02:46.143476Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"PUBGmodel = myNN(x_train)\nPUBGmodel.summary()","metadata":{"id":"NmlfXOi_Jykk","outputId":"f60fe1e5-c782-4625-d680-14739de8c074","execution":{"iopub.status.busy":"2022-08-13T02:02:46.148362Z","iopub.execute_input":"2022-08-13T02:02:46.149039Z","iopub.status.idle":"2022-08-13T02:02:46.323166Z","shell.execute_reply.started":"2022-08-13T02:02:46.148999Z","shell.execute_reply":"2022-08-13T02:02:46.321793Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ne = 5 #number of train loops\n\n#Train\ncallbacks=[keras.callbacks.ModelCheckpoint(\"pubg.keras\",save_best_only=True)]\nhistory=PUBGmodel.fit(x_train, y_train, epochs=ne, batch_size=128, verbose=1,validation_split=0.25,callbacks=callbacks)\nmodel = keras.models.load_model(\"pubg.keras\")","metadata":{"id":"ls1r0z627kvB","outputId":"3fdab0b6-bbfc-4c38-98a8-435c0dab847d","execution":{"iopub.status.busy":"2022-08-13T02:02:46.324677Z","iopub.execute_input":"2022-08-13T02:02:46.325066Z","iopub.status.idle":"2022-08-13T02:08:35.478362Z","shell.execute_reply.started":"2022-08-13T02:02:46.325026Z","shell.execute_reply":"2022-08-13T02:08:35.477007Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"hd = history.history\nprint(hd)\nloss_tr = hd['accuracy']\nloss_va = hd['val_accuracy']\nepochs = range(0, ne) #ne is number of epochs. Set it! \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nplt.plot(epochs, loss_tr, '-.o', label='Training Acc')\nplt.plot(epochs, loss_va, 'r', label='Validation Acc')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"id":"RefvAApD-JiZ","outputId":"71501b29-e1d4-44cf-a358-992253f306fe","execution":{"iopub.status.busy":"2022-08-13T02:08:35.482154Z","iopub.execute_input":"2022-08-13T02:08:35.482523Z","iopub.status.idle":"2022-08-13T02:08:36.111536Z","shell.execute_reply.started":"2022-08-13T02:08:35.482490Z","shell.execute_reply":"2022-08-13T02:08:36.110254Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the dense network just doesn't do that well. It's kind of horrible actually. It had an MAE of 0.16610 when submitted to the competition. It somehow beat a few peoples submissions but it's definitely not the best we can aim for. So let's move on to the tried and true regression models. ","metadata":{}},{"cell_type":"code","source":"#ytest=model.predict(xtest)","metadata":{"id":"j2NZN90TW6Lq","execution":{"iopub.status.busy":"2022-08-13T02:08:36.113058Z","iopub.execute_input":"2022-08-13T02:08:36.113602Z","iopub.status.idle":"2022-08-13T02:08:36.118777Z","shell.execute_reply.started":"2022-08-13T02:08:36.113563Z","shell.execute_reply":"2022-08-13T02:08:36.117615Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\ny_model = model.predict(x_train)\n#Calculate error and plot\nmse = mean_squared_error(y_train, y_model)\nmae = mean_absolute_error(y_train,y_model)\nprint(\"MSE: \", mse)\nprint(\"MAE: \", mae)\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-13T02:08:36.120182Z","iopub.execute_input":"2022-08-13T02:08:36.121120Z","iopub.status.idle":"2022-08-13T02:10:32.826114Z","shell.execute_reply.started":"2022-08-13T02:08:36.121084Z","shell.execute_reply":"2022-08-13T02:10:32.824922Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Here I tried applying our processed data into a very simple linear regression model. Not too much of interest going on here code wise. However...","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\nPUBGModel2=linear_model.LinearRegression()\nPUBGModel2.fit(x_train,y_train)\n#ytest=PUBGModel2.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T02:10:32.827567Z","iopub.execute_input":"2022-08-13T02:10:32.827960Z","iopub.status.idle":"2022-08-13T02:10:33.423855Z","shell.execute_reply.started":"2022-08-13T02:10:32.827925Z","shell.execute_reply":"2022-08-13T02:10:33.422303Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\ny_model = PUBGModel2.predict(x_train)\n#Calculate error and plot\nmse = mean_squared_error(y_train, y_model)\nmae = mean_absolute_error(y_train,y_model)\nprint(\"MSE: \", mse)\nprint(\"MAE: \", mae)\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-13T02:10:33.426379Z","iopub.execute_input":"2022-08-13T02:10:33.427517Z","iopub.status.idle":"2022-08-13T02:10:33.785617Z","shell.execute_reply.started":"2022-08-13T02:10:33.427451Z","shell.execute_reply":"2022-08-13T02:10:33.784329Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Our Linear Regression actually does worse in cross validation, but when submitted to the competition itself it nets a better score than that NN. ","metadata":{}},{"cell_type":"markdown","source":"This simple linear regression model was able to climb pretty well up the leaderboard alone with no other changes and just some data preprocessing. MAE: Score: 0.12912\n\nLet's try something else","metadata":{}},{"cell_type":"markdown","source":"Histogram-based Gradient Boosting Regression Tree.\nThe sklearn page for this model type credits its implementation to being inspired by LightGBM https://github.com/Microsoft/LightGBM.\n\nSo what is a Light Gradient Boosting Tree? It is a decision tree where multiple weak models are created, then combined to improve as we go. It has some similarities to random forests.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingRegressor\nPUBGM=HistGradientBoostingRegressor().fit(x_train,y_train)\nytest=PUBGM.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T02:10:33.787217Z","iopub.execute_input":"2022-08-13T02:10:33.788292Z","iopub.status.idle":"2022-08-13T02:11:01.595437Z","shell.execute_reply.started":"2022-08-13T02:10:33.788242Z","shell.execute_reply":"2022-08-13T02:11:01.594443Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\ny_model = PUBGM.predict(x_train)\n#Calculate error and plot\nmse = mean_squared_error(y_train, y_model)\nmae = mean_absolute_error(y_train,y_model)\nprint(\"MSE: \", mse)\nprint(\"MAE: \", mae)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T02:11:01.600219Z","iopub.execute_input":"2022-08-13T02:11:01.602882Z","iopub.status.idle":"2022-08-13T02:11:10.311571Z","shell.execute_reply.started":"2022-08-13T02:11:01.602838Z","shell.execute_reply":"2022-08-13T02:11:10.309503Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"This version of the model gave us a score of: 0.10597 closer to something greater, it also does better in cross validation than the other two models. ","metadata":{}},{"cell_type":"code","source":"df_submit = sample.copy()\ndf_submit['winPlacePerc']=ytest\ndf_submit.head()\ndf_submit.to_csv('submission.csv', index=False)","metadata":{"id":"etqYGVXkYNW0","execution":{"iopub.status.busy":"2022-08-13T02:11:10.312841Z","iopub.execute_input":"2022-08-13T02:11:10.313178Z","iopub.status.idle":"2022-08-13T02:11:14.954984Z","shell.execute_reply.started":"2022-08-13T02:11:10.313144Z","shell.execute_reply":"2022-08-13T02:11:14.953857Z"},"trusted":true},"execution_count":17,"outputs":[]}]}